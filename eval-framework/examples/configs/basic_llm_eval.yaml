# Basic LLM evaluation configuration
# Evaluates a language model on text generation tasks

model:
  type: text
  name: gpt-3.5-turbo
  batch_size: 8
  device: cuda:0
  parameters:
    temperature: 0.7
    max_tokens: 512
    top_p: 0.9

dataset:
  type: text
  name: alpaca_eval
  split: test
  max_samples: 1000
  batch_size: 8
  parameters:
    prompt_template: "Instruction: {instruction}\nInput: {input}\nOutput:"
    response_key: "output"

evaluator:
  type: llm_judge
  name: gpt-4-judge
  parameters:
    scoring_method: direct
    score_range: [1, 10]
    criteria:
      - accuracy
      - completeness
      - clarity
      - relevance
    bias_mitigation: true
    max_retries: 3

metrics:
  - name: accuracy
    type: exact_match
    parameters:
      normalize_text: true
      case_sensitive: false
  
  - name: rouge
    type: rouge
    parameters:
      metrics: ["rouge1", "rouge2", "rougeL"]
      use_stemmer: true
  
  - name: bertscore
    type: bertscore
    parameters:
      model_type: microsoft/deberta-xlarge-mnli
      batch_size: 8
      device: cuda:0

output:
  formats: ["html", "json"]
  save_predictions: true
  save_metrics: true
  save_error_analysis: true

logging:
  level: INFO
  save_logs: true
  log_dir: logs

seed: 42 