# Multimodal evaluation configuration
# Evaluates models that handle both text and images

model:
  type: multimodal
  name: flan-t5-xl-vit
  batch_size: 4
  device: cuda:0
  parameters:
    text_model:
      temperature: 0.7
      max_tokens: 512
      top_p: 0.9
    image_model:
      image_size: 224
      pretrained: true

dataset:
  type: multimodal
  name: coco_captions
  split: val
  max_samples: 500
  batch_size: 4
  parameters:
    image_dir: data/coco/images
    annotations_file: data/coco/annotations/captions_val2017.json
    image_transform:
      resize: [224, 224]
      normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

evaluator:
  type: multimodal
  name: clip_similarity
  parameters:
    model_name: openai/clip-vit-base-patch32
    similarity_threshold: 0.7
    modalities:
      - text
      - image
    metrics:
      - cosine_similarity
      - euclidean_distance

metrics:
  - name: clip_score
    type: clip
    parameters:
      model_name: openai/clip-vit-base-patch32
      batch_size: 4
      device: cuda:0
  
  - name: image_caption_bleu
    type: bleu
    parameters:
      n_gram: 4
      weights: [0.25, 0.25, 0.25, 0.25]
  
  - name: image_quality
    type: image_quality
    parameters:
      metrics:
        - ssim
        - psnr
        - fid
      reference_dir: data/reference_images

output:
  formats: ["html", "json", "pdf"]
  save_predictions: true
  save_metrics: true
  save_error_analysis: true
  save_visualizations: true
  visualization_dir: results/visualizations

logging:
  level: INFO
  save_logs: true
  log_dir: logs
  log_visualizations: true

seed: 42 